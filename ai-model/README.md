# **visign - Vietnamese Sign Language Recognition**
---

## Keypoints - MediaPipe

**Video:**
    - Aspect Ratio: 16:9 (1920Ã—1080)
    - FPS: 25â€“30 fps

**Keypoints Configuration**
- MediaPipe Holistic Configuration:
    * Hands Tracking: 21 x 2 hand landmarks
    * Pose Estimation: 33 landmarks
    * Face: 468 landmarks

- Frame: 150 frames (5 seconds @ 30 fps)
    * Resampling - Linear Interpolation

    *Input:* 114 frames
        Frame 0: hand at (x1, y1, z1)
        Frame 1: hand at (x2, y2, z2)
        ...
        Frame 113: hand at (x114, y114, z114)
    
    *Output:* 150 frames
        Frame 0: hand at (x1, y1, z1) - giá»¯ nguyÃªn
        Frame 1: hand at (x1.76, y1.76, z1.76) - interpolated
        Frame 2: hand at (x2.52, y2.52, z2.52) - interpolated
        ...
        Frame 149: hand at (x114, y114, z114) - giá»¯ nguyÃªn

UPPER_BODY_INDEXES = [
    0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
    11, 13, 15, 17, 19, 21,
    12, 14, 16, 18, 20, 22,
    23, 24
]

**src/keypoints/keypoints_extractor.py**

Extract Keypoints
```ruby
python src/keypoints/keypoints_extractor.py --process_dataset
```

Demo with single video
```ruby
python src/keypoints/keypoints_extractor.py <video_path> output_keypoints.npz
```

**Data Augmentation**
- *Linear Scaling:* Randomize scale factor from k_min to k_max
- 

**src/keypoints/augment.py**
Augmentation for ONE WORD
```ruby
python src/keypoints/augment.py "dataset/keypoints/ai cho" "augmented/ai cho" --n 50
```

Augmentation for DATASET
```ruby
python src/keypoints/augment.py dataset/keypoints augmented --n 50
```

```ruby
python src/keypoints/augment.py dataset/keypoints augmented --n 100 --kmin 0.8 --kmax 1.2 --sigma_body 0.02
```

**src/keypoints/keypoints_eval.py**
Evaluate augmented keypoints on video
```ruby
python src/keypoints/keypoints_eval.py "ai cho" --n_samples 10
```

**Output Structure:**
```
augmented/
â”œâ”€â”€ ai cho/
â”‚   â”œâ”€â”€ 0.npz (original)
â”‚   â”œâ”€â”€ 1.npz (augmented #1)
â”‚   â”œâ”€â”€ 2.npz (augmented #2)
â”‚   â””â”€â”€ ...
```

## Preprocess
- Load data from /augmented
    ```ruby
    pose: (150, 25, 3)    # 150 frames, 25 keypoints, 3 coords (x,y,z)
    left_hand: (150, 21, 3)
    right_hand: (150, 21, 3)
    face: (150, 468, 3)
    ```
- Detect missing hands
    * Check missing hand
    * Create mask: lh_mask, rh_mask (1=present, 0=missing)
    * Shape: (150,) - 1 value for each frame

- Normalize keypoints (center_and_scale):
    * Center: Trá»« keypoints theo shoulder midpoint
        shoulder_mid = (left_shoulder + right_shoulder) / 2
        Trá»« táº¥t cáº£ keypoints (pose, hands, face) theo center nÃ y
    * Scale: Chia theo shoulder distance (mean)
        scale = mean(||left_shoulder - right_shoulder||)
        Chia táº¥t cáº£ keypoints Ä‘á»ƒ Ä‘áº¡t scale-invariant
    * Chá»‰ dÃ¹ng x, y (bá» z)                      

## Model Architecture
- **Feature Builder:** Sau chuáº©n hoÃ¡, má»—i frame ghÃ©p `pose`, `left_hand`, `right_hand`, `face_subset`, kÃ¨m máº·t náº¡ hiá»‡n diá»‡n bÃ n tay (`lh_mask`, `rh_mask`) vÃ  vÃ©c-tÆ¡ váº­n tá»‘c. Chiá»u Ä‘áº·c trÆ°ng máº·c Ä‘á»‹nh: `pose 25Ã—2=50`, `hands 2Ã—21Ã—2=84`, `face_subset 89Ã—2=178`, `masks=2` â‡’ 314 chiá»u; báº­t velocity â†’ 628 chiá»u.
- **Projection Layer:** `nn.Linear(in_feat â†’ 256)` + LayerNorm + ReLU + Dropout Ä‘á»ƒ gom Ä‘áº·c trÆ°ng khung hÃ¬nh vá» khÃ´ng gian chung.
- **BiLSTM Encoder:** 2 táº§ng LSTM hai chiá»u (hidden 256, dropout 0.35) giÃºp mÃ´ hÃ¬nh báº¯t tÃ­n hiá»‡u thá»i gian thuáº­n/ngÆ°á»£c.
- **Attention Pooling:** Lá»›p `AttentionPooling` há»c trá»ng sá»‘ theo thá»i gian (mask cÃ¡c frame bÃ n tay máº¥t) â†’ vector cÃ¢u toÃ n cá»¥c.
- **Classifier Head:** `Linear(512â†’256) â†’ ReLU â†’ Dropout â†’ Linear(256â†’#classes)` vá»›i label smoothing & class weight tuá»³ chá»n.
- **Checkpoint Artifacts:** `artifacts/best_model.pt` lÆ°u `state_dict`, `label2idx`, cáº¥u hÃ¬nh model; `training_history.json` ghi láº¡i loss/acc/F1 tá»«ng epoch.

## Training Pipeline
1. **Sinh keypoints vÃ  augmentation**
   - TrÃ­ch xuáº¥t keypoints: `python src/keypoints/keypoints_extractor.py --process_dataset`
   - TÄƒng cÆ°á»ng dá»¯ liá»‡u: `python src/keypoints/augment.py dataset/keypoints augmented --n 50`

2. **XÃ¢y dá»±ng chá»‰ má»¥c & tiá»n xá»­ lÃ½**
   - Táº¡o `index.csv` & Ä‘áº·c trÆ°ng numpy: `python src/train/preprocess_pipeline.py` (máº·c Ä‘á»‹nh Ä‘á»c `augmented/`, sinh `index.csv` + `preprocessed_npz/sample_{i}_{label}.npy`). Náº¿u thÆ° má»¥c khÃ¡c, sá»­a biáº¿n trong script hoáº·c cháº¡y theo module cÃ³ Ä‘á»‘i sá»‘ tuá»³ biáº¿n.

3. **Huáº¥n luyá»‡n**
   ```bash
   python src/train/modeling.py \
     --index-csv index.csv \
     --feature-dir preprocessed_npz \
     --output-dir artifacts \
     --epochs 60 \
     --batch-size 32 \
     --lr 1e-3 \
     --use-class-weights \
     --label-smoothing 0.05
   ```
   - Tham sá»‘ quan trá»ng: `--proj-dim`, `--hidden-size`, `--num-layers`, `--no-attention`, `--no-velocity`, `--val-ratio`, `--patience`, `--device`.
   - Trong training: in ra `train_loss/val_loss`, `train_f1/val_f1`, Ä‘iá»u chá»‰nh LR báº±ng ReduceLROnPlateau, early stopping.

4. **ÄÃ¡nh giÃ¡**
   - Má»—i epoch in F1 macro & Accuracy trÃªn `val_loader`.
   - Model tá»‘t nháº¥t (theo `val_f1`) Ä‘Æ°á»£c ghi Ä‘Ã¨ `artifacts/best_model.pt` cÃ¹ng metric snapshot.

## Inference & Demo

### Web Application (FastAPI)

á»¨ng dá»¥ng web Ä‘Æ¡n giáº£n Ä‘á»ƒ há»c ngÃ´n ngá»¯ kÃ½ hiá»‡u vá»›i video hÆ°á»›ng dáº«n vÃ  webcam.

**CÃ i Ä‘áº·t dependencies:**
```bash
pip install fastapi uvicorn jinja2 python-multipart
```

**Cháº¡y á»©ng dá»¥ng:**
```bash
# Windows
python app.py

# Hoáº·c sá»­ dá»¥ng uvicorn trá»±c tiáº¿p
uvicorn app:app --reload --host 0.0.0.0 --port 8000
```

**Truy cáº­p:** Má»Ÿ trÃ¬nh duyá»‡t vÃ  vÃ o `http://localhost:8000`

**TÃ­nh nÄƒng:**
- **Video hÆ°á»›ng dáº«n:** Hiá»ƒn thá»‹ video Vimeo tá»« `data/cleaned_data.csv`
- **Webcam:** Stream camera cá»§a ngÆ°á»i dÃ¹ng theo thá»i gian thá»±c
- **Chá»n tá»« vá»±ng:** Dropdown Ä‘á»ƒ chá»n theo chá»§ Ä‘á» (Topic) vÃ  tá»« cá»¥ thá»ƒ (Label)
- **PhÃ¡t láº¡i video:** NÃºt Ä‘á»ƒ phÃ¡t láº¡i video hÆ°á»›ng dáº«n
- **Ghi 5 giÃ¢y:** NÃºt "Báº¯t Äáº§u Ghi" Ä‘á»ƒ ghi láº¡i video tá»« webcam trong 5 giÃ¢y

**LÆ°u Ã½:** 
- TrÃ¬nh duyá»‡t sáº½ yÃªu cáº§u quyá»n truy cáº­p camera khi má»Ÿ trang
- Video Ä‘Æ°á»£c ghi sáº½ Ä‘Æ°á»£c lÆ°u dÆ°á»›i dáº¡ng Blob trong bá»™ nhá»› trÃ¬nh duyá»‡t

### Deploy lÃªn Production

Xem file `DEPLOY.md` Ä‘á»ƒ biáº¿t hÆ°á»›ng dáº«n chi tiáº¿t deploy lÃªn Railway, Render hoáº·c Vercel.

**Nhanh nháº¥t vá»›i Railway:**
1. ÄÄƒng kÃ½ táº¡i [railway.app](https://railway.app)
2. Connect GitHub repo
3. Railway tá»± Ä‘á»™ng deploy
4. Done! ğŸ‰